{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2-WDaugherty/blob/main/2_size_estimator_and_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1A-uzhwpt_"
      },
      "source": [
        "# **2. Model Size Estimation**\n",
        "\n",
        "It is no surprise that with such a tiny package, your Ardunio Nano 33 BLE Sense comes with limited memory and processing power. Therefore, you must be aware of the size and components of your model in order to have it run efficiently on your MCU.\n",
        "\n",
        "This notebook explores how various neural network layers affect the number of parameters, the amount memory, the number of floating point operations, and the CPU runtime of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 2.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BgbZjaQZ8niT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd3cd99-716d-4398-c269-3b63fc356469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sH-xe50YtFNd"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rRj2U4Y3ttgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811115ef-7c39-4f86-ae09-1d14b89c4bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a2-WDaugherty' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a2-WDaugherty\n",
            "M\t2_size_estimator_and_profiler.ipynb\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "Already up to date.\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R9V2uP4YtzuR"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIzgKXcDtFNe"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ePTgb55gwT7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1daf0c-6005-47ed-86e9-ab5d05ba3a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a2-WDaugherty\n",
            "bonus_constants.py  data_proc.py  __pycache__\t       size_estimate.py\n",
            "bonus_quant.py\t    loaders.py\t  quant_conversion.py  train_val_test_utils.py\n",
            "constants.py\t    networks.py   quant.py\n",
            "['/content', '/env/python', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.9/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(PROJECT_ROOT)\n",
        "!ls {PROJECT_ROOT}/src\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2uKEHl-OtFNf",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818ee725-d055-4edb-993a-f3c4d9a812e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folders are created, \n",
            "PyTorch models will be saved in /content/gdrive/MyDrive/ece5545/models/torch_models, \n",
            "ONNX models will be saved in /content/gdrive/MyDrive/ece5545/models/onnx_models, \n",
            "TensorFlow Saved Models will be saved in /content/gdrive/MyDrive/ece5545/models/tf_models, \n",
            "TensorFlow Lite models will be saved in /content/gdrive/MyDrive/ece5545/models/tflite_models, \n",
            "TensorFlow Lite Micro models will be saved in /content/gdrive/MyDrive/ece5545/models/micro_models.\n",
            "Imported code dependencies\n"
          ]
        }
      ],
      "source": [
        "import sys,os\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nnt\n",
        "import src.data_proc as data_proc\n",
        "from src.constants import *\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imported code dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTKG-QIfwcN9"
      },
      "source": [
        "## 2.2 Define the Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRSPvUTyIQ2"
      },
      "source": [
        "### Create the model\n",
        "Our TinyConv model currently consists of 7 layers:\n",
        "\n",
        "\n",
        "1. [Reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "2. [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "3. [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) \n",
        "4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) \n",
        "5. Reshape\n",
        "6. [Fully Connected (Linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "7. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)\n",
        "\n",
        "\n",
        "Please refer to `<github_dir>/src/networks.py` for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jU2ryBhlwcN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1592cb-3505-41ca-cacb-d7dcb2796c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu to run the training scrpit.\n"
          ]
        }
      ],
      "source": [
        "# Define device\n",
        "from src.networks import TinyConv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLwsdGt_liKa"
      },
      "source": [
        "### Create data_proc.AudioProcessor() object for data preprocessing\n",
        "When an AudioProcessor instance is created: \n",
        "\n",
        "1. Download speech_command dataset from DATA_URL (defined in constants.py) to data_dir (default: '/content/gdrive/MyDrive/ece5545/data')\n",
        "default dataset url: 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "\n",
        "2. Determine classes and their numerical indices for training and testing based on WANTED_WORDS \n",
        "(defined in constants.py): \n",
        "eg. if WANTED_WORDS is ['yes', 'no'], model will be trained to identify \"yes\" and \"no\" as yes and no, \n",
        "other words as unkown, and background noises as silence\n",
        "\n",
        "3. Determine and save the settings for data processing feature generator based on relavent constants \n",
        "in constants.py\n",
        "\n",
        "4. Determine which audio files in the dataset are for testing, training, or validating using hash method\n",
        "\n",
        "5. Prepare and save background noise data using the background noise data inside dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HLxcu6BB_wxg"
      },
      "outputs": [],
      "source": [
        "# Create audio processor (this takes some time the first time)\n",
        "# And continues to run for a bit after reaching 100% while it's extracting files\n",
        "audio_processor = data_proc.AudioProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_RJ8a0otJEJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db8d8b1-122b-4c5d-d1ce-a9442f26777e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Create model\n",
        "model_fp32 = TinyConv(audio_processor.model_settings)\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcB7Ri8ewcOA"
      },
      "source": [
        "## 2.3 Model Estimates\n",
        "Run the next few cells to see how each layer impacts memory and runtime of the below TinyConv neural network model. Then experiment with reshaping it to see how adding or removing layers alters the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEnwhmThwcOC"
      },
      "source": [
        "### Memory Utilization\n",
        "\n",
        "There are two important forms of memory that we care about for MCUs: **flash memory** and **random access memory (RAM)**. Flash is **non-volatile** aka persistent storage memory; its data is saved when powered off. This is where your model's weights and code live, thus they must be able to fit within the capacity of your MCU's flash memory (1MB). On the other hand, RAM is **volatile** or non-persistent memory, thus it is used for temporary storage like input buffers and intermediate tensors. Together, they cannot exceed the size of your RAM storage (256KB).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Gv8LHUtFNi"
      },
      "source": [
        "### TODO 1: Implement the `count_trainable_parameters` function in `src/size_estimate.py` to compute model size and get an estimate of the flash usage of this model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pulls the neest version of the repo\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-WDaugherty/src\n",
        "!git pull "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpmmWHDZEFGe",
        "outputId": "e20b04a2-4a40-48d4-949c-af1b803722b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a2-WDaugherty/src\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "--J01LrjtFNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b653ca9-ac4d-4f38-e859-bbbdc23bcd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters:  0.016652 M\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import count_trainable_parameters\n",
        "num_params = count_trainable_parameters(model_fp32)\n",
        "print(\"Total number of trainable parameters: \", num_params / float(1e6), \"M\") # Should be about 0.016652 M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZaeZSLrtFNi"
      },
      "source": [
        "### TODO 2: Implement the `compute_forward_size` function in `src/size_estimate.py` to compute the memory needed for a forward pass. This is how much RAM you will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tLJ-LBfo56IL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e5d25b-48ff-4673-eb26-9607a1a246fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward memory:  0.007856 M\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import compute_forward_memory\n",
        "frd_memory = compute_forward_memory(\n",
        "    model_fp32,\n",
        "    (1, model_fp32.model_settings['fingerprint_width'], model_fp32.model_settings['spectrogram_length']),\n",
        "    device\n",
        ")\n",
        "print(\"Forward memory: \", frd_memory / float(1e6), \"M\") # Should be about 0.03462 M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjm2OlXgArr"
      },
      "source": [
        "As you can see above, the number of parameters in a neural network can add up fast which is a concern when dealing with a small amount of RAM. With the TinyConv neural network only consuming 0.21MB out of 1MB, our model will easily fit within flash memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qkx4L8hCBD"
      },
      "source": [
        "### Number of Operations\n",
        "\n",
        "### TODO 3: Implement the `flop` function in `src/size_estimate.py` to count the total FLOPS in a forward pass with batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Mb5ugZA3wcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6c85f8-e637-42b3-d2d8-665d3675f5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of floating operations: 1316004\n",
            "Number of FLOPs by layer and parameters:\n",
            "Conv:  {'bias': 4000, 'weight': 1280000}\n",
            "FC:    {'bias': 4, 'weight': 32000}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from src.size_estimate import flop\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "# The total number of floating point operations \n",
        "flop_by_layers = flop(\n",
        "    model=model_fp32, \n",
        "    input_shape=(\n",
        "        1, \n",
        "        model_fp32.model_settings['fingerprint_width'], \n",
        "        model_fp32.model_settings['spectrogram_length']\n",
        "    ), \n",
        "    device=device)\n",
        "total_param_flops = sum([sum(val.values()) for val in flop_by_layers.values()])\n",
        "\n",
        "\n",
        "print(f'total number of floating operations: {total_param_flops}')  # total number of floating operations: 340004\n",
        "print('Number of FLOPs by layer and parameters:') \n",
        "print(\"Conv: \", flop_by_layers['conv'])  # {'bias': 4000, 'weight': 320000} divide by 2\n",
        "print(\"FC:   \", flop_by_layers['fc'])  # {'bias': 4, 'weight': 16000} divide by 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAbpr4Mj12i"
      },
      "source": [
        "### CPU runtime\n",
        "\n",
        "### TODO 4: Measure the server/desktop CPU runtime to compare to the MCU runtime later in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zMcAtzKHwcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be2cc2f-0600-4e46-a796-bcefea034af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             model_inference         0.41%     644.000us        98.68%     154.655ms     154.655ms             1            --  \n",
            "                aten::conv2d         0.00%       7.000us        81.62%     127.929ms     127.929ms             1       640.000  \n",
            "           aten::convolution         0.02%      33.000us        81.62%     127.922ms     127.922ms             1            --  \n",
            "          aten::_convolution         0.01%      19.000us        81.60%     127.889ms     127.889ms             1            --  \n",
            "    aten::mkldnn_convolution        81.55%     127.815ms        81.59%     127.870ms     127.870ms             1            --  \n",
            "                aten::linear         0.01%      19.000us         8.18%      12.827ms      12.827ms             1            --  \n",
            "                 aten::addmm         6.05%       9.481ms         8.16%      12.791ms      12.791ms             1        32.000  \n",
            "               aten::softmax         1.92%       3.004ms         4.21%       6.598ms       6.598ms             1            --  \n",
            "                  aten::relu         0.03%      44.000us         4.13%       6.475ms       6.475ms             1            --  \n",
            "             aten::clamp_min         4.10%       6.431ms         4.10%       6.431ms       6.431ms             1            --  \n",
            "              aten::_softmax         2.29%       3.594ms         2.29%       3.594ms       3.594ms             1            --  \n",
            "                 aten::copy_         2.08%       3.262ms         2.08%       3.262ms       3.262ms             1            --  \n",
            "                 aten::zeros         1.32%       2.062ms         1.32%       2.075ms       2.075ms             1            --  \n",
            "               aten::reshape         0.10%     151.000us         0.11%     177.000us      88.500us             2            --  \n",
            "                aten::expand         0.03%      45.000us         0.03%      47.000us      47.000us             1            --  \n",
            "           aten::as_strided_         0.02%      39.000us         0.02%      39.000us      39.000us             1            --  \n",
            "                 aten::empty         0.02%      33.000us         0.02%      33.000us       8.250us             4            --  \n",
            "        aten::_reshape_alias         0.02%      26.000us         0.02%      26.000us      13.000us             2            --  \n",
            "                     aten::t         0.01%      10.000us         0.01%      17.000us      17.000us             1            --  \n",
            "             aten::transpose         0.00%       4.000us         0.00%       7.000us       7.000us             1            --  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 156.730ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_fp32.cpu()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cpu()\n",
        "\n",
        "# Run a profiler to see the cpu time for inference \n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modified profiler to show cuda and cpu times\n",
        "model_fp32.cuda()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cuda()\n",
        "\n",
        "# Run a profiler to see the gpu time for inference \n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
      ],
      "metadata": {
        "id": "xP1BVEScucMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472289cc-dd23-4cad-f479-ea6ab53956e2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "void implicit_convolve_sgemm<float, float, 128, 5, 5...         0.00%       0.000us         0.00%       0.000us       0.000us      18.000us        41.86%      18.000us      18.000us             1  \n",
            "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        13.95%       6.000us       6.000us             1  \n",
            "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us        11.63%       5.000us       5.000us             1  \n",
            "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us        11.63%       5.000us       5.000us             1  \n",
            "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us        11.63%       5.000us       5.000us             1  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         9.30%       4.000us       4.000us             1  \n",
            "                                  cudaStreamIsCapturing         1.91%       4.000us         1.91%       4.000us       4.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                  cudaStreamGetPriority         0.96%       2.000us         0.96%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.48%       1.000us         0.48%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                       cudaLaunchKernel        83.25%     174.000us        83.25%     174.000us      29.000us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         5.74%      12.000us         5.74%      12.000us      12.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                  cudaDeviceSynchronize         7.66%      16.000us         7.66%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 209.000us\n",
            "Self CUDA time total: 43.000us\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "92bf126df007708fd70c442c808ee74575bedf7ea6317e0b182c3af0184af25d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}